{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI6tgh0cvgUh",
        "outputId": "d24712b0-38d2-4095-c381-b183fae605c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh3EGyqPmvlD"
      },
      "source": [
        "# Transfer learning\n",
        "\n",
        "Cuando hablamos de transfer learning nos referimos al proceso de entrenamiento de un modelo en un problema relacionado con otro modelo. La manera en que funciona es cuando tomamos los pesos ya entrenados de una arquitectura de red neuronal que fue entrenada con una gran cantidad de imagenes. \n",
        "\n",
        "Estos pesos reusados pueden ayudarnos a resolver el problema, o pueden ser un punto de partida para entrenar nuestro propio modelo y adaptarlo a un nuevo problema.\n",
        "\n",
        "Por lo general nos encontramos con librerias que ya tienen incluidas aplicaciones con estas arquitecturas, en donde se entrenaron los pesos con la base de datos IMAGENET.\n",
        "\n",
        "## VGG16\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8tCsX47ms6T",
        "outputId": "96063b5e-5757-4ea5-a64a-ba1b19886511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 4096)              102764544 \n",
            "                                                                 \n",
            " fc2 (Dense)                 (None, 4096)              16781312  \n",
            "                                                                 \n",
            " predictions (Dense)         (None, 1000)              4097000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#vgg16 model\n",
        "from keras.applications.vgg16 import VGG16\n",
        "# load model\n",
        "model = VGG16()\n",
        "# summarize the model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwAnMufHrVBv"
      },
      "source": [
        "El siguiente ejemplo fue sacado de un curso tomado para redes neuronales. \n",
        "\n",
        "\n",
        "Como primer paso se importa la libreria `glob` la cual nos ayudara a importar las imagenes por categoria, y modificarlas para poder usar el transfer learning.\n",
        "\n",
        "Como regla inicial para todas las arquitecturas, el tamano de las imagenes debe de ser 224x224.\n",
        "\n",
        "En el dataset sólo tenemos dos clases las cuales se definen acontiuacion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2KCiCiiiez-l"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "y3fwpes9ra2d"
      },
      "outputs": [],
      "source": [
        "# For training set only\n",
        "import glob\n",
        "organic = glob.glob('C:/Git/MCD-PCD/Tarea 06/datos/DATASET/DATASET/TRAIN/O/*.*')\n",
        "recycle = glob.glob('C:/Git/MCD-PCD/Tarea 06/datos/DATASET/DATASET/TRAIN/C/*.*')\n",
        "# angry = glob.glob('/content/drive/My_Drive/train_logmel/angry/*.*')\n",
        "# calm = glob.glob('/content/drive/My_Drive/train_logmel/calm/*.*')\n",
        "# disgust = glob.glob('/content/drive/My_Drive/train_logmel/disgust/*.*')\n",
        "# fearful = glob.glob('/content/drive/My_Drive/train_logmel/fearful/*.*')\n",
        "# happy = glob.glob('/content/drive/My_Drive/train_logmel/happy/*.*')\n",
        "# neutral = glob.glob('/content/drive/My_Drive/train_logmel/neutral/*.*')\n",
        "# sad = glob.glob('/content/drive/My_Drive/train_logmel/sad/*.*')\n",
        "# surprised = glob.glob('/content/drive/My_Drive/train_logmel/surprised/*.*')\n",
        "data = []\n",
        "labels = []\n",
        "for i in organic:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n",
        "    target_size= (224,224))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append('organic')\n",
        "for i in recycle:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n",
        "    target_size= (224,224))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append('recycle')\n",
        "# for i in angry:   \n",
        "#     image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n",
        "#     target_size= (224,224))\n",
        "#     image=np.array(image)\n",
        "#     data.append(image)\n",
        "#     labels.append('Angry')\n",
        "# for i in calm:   \n",
        "#     image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n",
        "#     target_size= (224,224))\n",
        "#     image=np.array(image)\n",
        "#     data.append(image)\n",
        "#     labels.append('Calm')\n",
        "# for i in disgust:   \n",
        "#     image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n",
        "#     target_size= (224,224))\n",
        "#     image=np.array(image)\n",
        "#     data.append(image)\n",
        "#     labels.append('Disgust')\n",
        "# for i in fearful:   \n",
        "#     image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n",
        "#     target_size= (224,224))\n",
        "#     image=np.array(image)\n",
        "#     data.append(image)\n",
        "#     labels.append('Fearful')\n",
        "# for i in happy:   \n",
        "#     image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n",
        "#     target_size= (224,224))\n",
        "#     image=np.array(image)\n",
        "#     data.append(image)\n",
        "#     labels.append('Happy')\n",
        "# for i in neutral:   \n",
        "#     image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n",
        "#     target_size= (224,224))\n",
        "#     image=np.array(image)\n",
        "#     data.append(image)\n",
        "#     labels.append('Neutral')\n",
        "# for i in sad:   \n",
        "#     image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n",
        "#     target_size= (224,224))\n",
        "#     image=np.array(image)\n",
        "#     data.append(image)\n",
        "#     labels.append('Sad')\n",
        "# for i in surprised:   \n",
        "#     image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n",
        "#     target_size= (224,224))\n",
        "#     image=np.array(image)\n",
        "#     data.append(image)\n",
        "#     labels.append('Surprised')\n",
        "train_data = np.array(data)\n",
        "train_labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF1ycCxlrfcs"
      },
      "source": [
        "Normalizamos los datos y hacemos una transformacion a las etiquetas que pusimos anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CoROJS9bIl4a"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "\n",
        "(X_train, X_test, y_train, y_test) = train_test_split(train_data, train_labels, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.layers import Input, Flatten, Dense, Dropout\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "84caHXtHrdi7"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "# le = preprocessing.LabelEncoder()\n",
        "# le.fit([\"organic\", \"recycle\"])\n",
        "lb = preprocessing.LabelEncoder()\n",
        "y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
        "y_test = np_utils.to_categorical(lb.fit_transform(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqUoOKzQraCO"
      },
      "source": [
        "Importamos desde la aplicacion de Keras los pesos que vamos a utilizar, en este caso vamos a usar la arquitectura de VGG16.\n",
        "\n",
        "Cuando corran este parte del programa les debe de dar como salida las capas entrenables. En caso de que no quieran moverle a los pesos, todas deben de estar puestas como False."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nIb1vxKZr6hp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 input_2 False\n",
            "1 block1_conv1 False\n",
            "2 block1_conv2 False\n",
            "3 block1_pool False\n",
            "4 block2_conv1 False\n",
            "5 block2_conv2 False\n",
            "6 block2_pool False\n",
            "7 block3_conv1 False\n",
            "8 block3_conv2 False\n",
            "9 block3_conv3 False\n",
            "10 block3_pool False\n",
            "11 block4_conv1 False\n",
            "12 block4_conv2 False\n",
            "13 block4_conv3 False\n",
            "14 block4_pool False\n",
            "15 block5_conv1 False\n",
            "16 block5_conv2 False\n",
            "17 block5_conv3 False\n",
            "18 block5_pool False\n"
          ]
        }
      ],
      "source": [
        "from keras.applications import VGG16\n",
        "vgg_model = VGG16(weights='imagenet',include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "for layer in vgg_model.layers:\n",
        "    layer.trainable = False\n",
        "# Make sure you have frozen the correct layers\n",
        "for i, layer in enumerate(vgg_model.layers):\n",
        "    print(i, layer.name, layer.trainable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXi2zTP0sSsR"
      },
      "source": [
        "Ahora tenemos las capas que podemos modificar. Recuerden que estas se agregan solo en caso de que las imagenes con las que esten trabajando sean muy diferentes de las que se encuentran en el IMAGENET.\n",
        "\n",
        "Se especifica en la ultima capa densa cuantas clases esperan tener. En este caso ponemos 8 por 8 clases distintas. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CfRSSw89r9RU"
      },
      "outputs": [],
      "source": [
        "x = vgg_model.output\n",
        "x = Flatten()(x) # Flatten dimensions to for use in FC layers\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x) # Dropout layer to reduce overfitting\n",
        "x = Dense(256, activation='relu')(x)\n",
        "# x = Dense(8, activation='softmax')(x) # Softmax for multiclass\n",
        "x = Dense(1, activation='softmax')(x) # Softmax for multiclass\n",
        "transfer_model = Model(inputs=vgg_model.input, outputs=x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRStyDVRr-Km"
      },
      "source": [
        "Compilando el modelo:\n",
        "Parece que algo no está bien configurado porque el modelo muestra una eficacia del 100%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras import optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dpHxqkbGsrO6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Git\\MCD-PCD\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "9423/9423 [==============================] - 1859s 197ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 2/3\n",
            "9423/9423 [==============================] - 1779s 189ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 3/3\n",
            "9423/9423 [==============================] - 1741s 185ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "learning_rate= 5e-5\n",
        "transfer_model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=learning_rate), metrics=[\"accuracy\"])\n",
        "history = transfer_model.fit(X_train, y_train, batch_size = 1, epochs=3, validation_data=(X_test,y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akzVafGItBta"
      },
      "source": [
        "Grafica el rendimiento.\n",
        "Parece que algo esta mal porque imprime una sola línea recta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot Loss and Accuracies\n",
        "\n",
        "# loss\n",
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.plot(history.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Tarea 07 - Transfer_learning.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.11 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "3c45d73fd70bd870537f116763037b08300eb8e52ee46eae169a00e72a03bc3f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
